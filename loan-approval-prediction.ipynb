{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan Approval Prediction: \n### EDA + Decision Tree, Random Forest & Logistic Regression Modeling","metadata":{"_uuid":"85300d266ca673fec60d4d093668e4dff1a09861"}},{"cell_type":"code","source":"################### Importing Libraries ######################\nimport pandas as pd\n\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.info()","metadata":{"_uuid":"2e636af60c64ce6e4ab9eb2e673c431b2428d8bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations:\n1. We can see there are total 13 columns including target variable, all of them are self explanatory. \n2. We also see some missing values, lets take stock of missing columns and what are the possible values for categorical and numerical columns \n","metadata":{"_uuid":"4a4697b281e0f2fd86b17b298186ecceddb5a0fe"}},{"cell_type":"code","source":"############ Count number of Categorical and Numerical Columns ######################\ntrain_df = train_df.drop(columns=['Loan_ID']) ## Dropping Loan ID\ncategorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']\n\nprint(categorical_columns)\nnumerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\nprint(numerical_columns)\n","metadata":{"_uuid":"52bb8de43b1cad0bb0036d7847c7bde259927692"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Analyze values assigned to columns ","metadata":{"_uuid":"23045afec9e84dcfd7abcbe0cc82b0e14a149f7b"}},{"cell_type":"code","source":"### Data Visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfig,axes = plt.subplots(4,2,figsize=(12,15))\nfor idx,cat_col in enumerate(categorical_columns):\n    row,col = idx//2,idx%2\n    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col])\n\n\nplt.subplots_adjust(hspace=1)","metadata":{"_uuid":"eb1e277cafbb1d8469df6fed6a7c5c89a6d32d58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plots above convey following things about the dataset:\n1. Loan Approval Status: About 2/3rd of applicants have been granted loan.\n2. Sex: There are more Men  than Women (approx. 3x) \n3. Martial Status: 2/3rd of the population in the dataset is Marred; Married applicants are more likely to be granted loans.\n4. Dependents: Majority of the population have zero dependents and are also likely to accepted for loan.\n5. Education: About 5/6th of the population is Graduate and graduates have higher propotion of loan approval\n6. Employment: 5/6th of population is not self employed.\n7. Property Area: More applicants from Semi-urban and also likely to be granted loans.\n8. Applicant with credit history are far more likely to be accepted.\n9. Loan Amount Term: Majority of the loans taken are for 360 Months (30 years).\n\nNow, let's also analyze Numerical Columns:","metadata":{"_uuid":"d762772e396fb9e418c4681e67577ce66ab4a62f"}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,3,figsize=(17,5))\nfor idx,cat_col in enumerate(numerical_columns):\n    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx])\n\nprint(train_df[numerical_columns].describe())\nplt.subplots_adjust(hspace=1)","metadata":{"_uuid":"605f3e7876f51748af7c2c5588b7592691151667"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Numercical Columns, there is no significant relation to Loan approval status.\n","metadata":{"_uuid":"7b6a6d6b728423a1da70b25d8943eec34a9771ac"}},{"cell_type":"markdown","source":"### Preprocessing Data:\nInput data needs to be pre-processed before we feed it to model. Following things need to be taken care:\n1. Encoding Categorical Features.\n2. Imputing missing values","metadata":{"_uuid":"f9e340ec6989242f4276872955ae5fd44a136c55"}},{"cell_type":"code","source":"#### Encoding categrical Features: ##########\ntrain_df_encoded = pd.get_dummies(train_df,drop_first=True)\ntrain_df_encoded.head()","metadata":{"_uuid":"37ba5a08c0113aa836127ea0199f40315e5ecd9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Split Features and Target Varible ############\nX = train_df_encoded.drop(columns='Loan_Status_Y')\ny = train_df_encoded['Loan_Status_Y']\n\n################# Splitting into Train -Test Data #######\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)\n############### Handling/Imputing Missing values #############\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\nimp_train = imp.fit(X_train)\nX_train = imp_train.transform(X_train)\nX_test_imp = imp_train.transform(X_test)\n","metadata":{"_uuid":"c2fb4b1de74e27e787477900233b3122fcdbae6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 1: Decision Tree Classifier\n","metadata":{"_uuid":"2d12310c8bae698527bd399ecad2e5bbf3af36a0"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\n\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_train)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"_uuid":"652d6255869bb59a347610940549fe7688f23225"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Overfitting Problem\nWe can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.\n\n#### First let's try tuning 'Max_Depth' of tree\n","metadata":{"_uuid":"8b1e20932d504ecfdd5d6c9342039a67ad5c63cb"}},{"cell_type":"code","source":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\n\nfor depth in range(1,20):\n    tree_clf = DecisionTreeClassifier(max_depth=depth)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n    \n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)\n","metadata":{"_uuid":"67a7c9c2b842e3a0d17fba4e73be73f778c56ab8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score\nOptimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7\n#### Visulazing Decision Tree with Max Depth = 3","metadata":{"_uuid":"1feb6093458b6b30a8135902b0ea2cd6687d51ae"}},{"cell_type":"code","source":"\nimport graphviz \nfrom sklearn import tree\n\ntree_clf = tree.DecisionTreeClassifier(max_depth = 3)\ntree_clf.fit(X_train,y_train)\ndot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())\ngraph = graphviz.Source(dot_data)\ngraph","metadata":{"_uuid":"2027e4690649174f65ec06eeb5e295004b28f8f8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above tree, we could see that some of the leafs have less than 5 samples hence our classifier might overfit.\nWe can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 3","metadata":{"_uuid":"049d79525bed8f9c6539fa00e986fabca839c36b"}},{"cell_type":"code","source":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\nmin_samples_leaf = []\nimport numpy as np\nfor samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    min_samples_leaf.append(samples_leaf)\n    \n\nTuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\nTuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n\nplot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"_uuid":"c86f55903a45abe87fb749f82ad24cfffbe547ae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above plot, we will choose Min_Samples_leaf to 35 to improve test accuracy. \n\nLet's use this Decision Tree classifier on unseen test data and evaluate __Test Accuracy, F1 Score and Confusion Matrix__","metadata":{"_uuid":"fe3f6c814829be8931832a63b3a920f4623c45f7"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ntree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)\ntree_clf.fit(X_train,y_train)\ny_pred = tree_clf.predict(X_test_imp)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"_uuid":"5e66c2b2351d8ce910bc3faf04ad75d1abd65e15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  Mis-classifications\nIt can be seen that majority of the misclassifications are happening because of Loan Reject applicants being classified as Accept.\n\nLet's look into Random Forest Classifier if it can reduce mis-classifications","metadata":{"_uuid":"483827c921a79654efbc26a5e5e19cd74cdb56a1"}},{"cell_type":"markdown","source":"### Model 2: Random Forest Classifier\n","metadata":{"_uuid":"f87b1c06ede59dceb27c45617d1b3d5b2f94a5c3"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)\nrf_clf.fit(X_train,y_train)\ny_pred = rf_clf.predict(X_train)\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())\n","metadata":{"_uuid":"5fab14ff1b38e4b6b8f57b6f49605313bb82dbb7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest: Test Data Evaluation","metadata":{"_uuid":"2f6d57ca1ae24e5e562d7b14494f3c0361a2a9d8"}},{"cell_type":"code","source":"y_pred = rf_clf.predict(X_test_imp)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"_uuid":"81ecfeffb42e244bed64392387b6145a843bbd2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest gives same results as Decision Tree Classifier.\nFinally, we will try Logistic Regression Model by sweeping threshold values.","metadata":{"_uuid":"ff51ca5edfc4dc669cb29ff35c39bf42d788bbcd"}},{"cell_type":"markdown","source":"### Model 3: Logistic Regression","metadata":{"_uuid":"0865f5b0d5a38ff6a12b656e391c7ab3f7a59597"}},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\n\ntrain_accuracies = []\ntrain_f1_scores = []\ntest_accuracies = []\ntest_f1_scores = []\nthresholds = []\n\n#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\nfor thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n    logreg_clf = LogisticRegression(solver='liblinear')\n    logreg_clf.fit(X_train,y_train)\n    \n    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n\n    train_acc = accuracy_score(y_train,y_pred_train)\n    train_f1 = f1_score(y_train,y_pred_train)\n    \n    y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n    \n    test_acc = accuracy_score(y_test,y_pred_test)\n    test_f1 = f1_score(y_test,y_pred_test)\n    \n    train_accuracies.append(train_acc)\n    train_f1_scores.append(train_f1)\n    test_accuracies.append(test_acc)\n    test_f1_scores.append(test_f1)\n    thresholds.append(thresh)\n    \n    \nThreshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\nThreshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n\nplot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"_uuid":"6d26944c677c8b8123814838192deccaeb1e61e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression does slightly better than Decision Tree and Random Forest.\n<br> Based on the above Test/Train curves, we can keep threshold to 0.4. <br>\nNow Finally let's look at Logistic Regression Confusion Matrix","metadata":{"_uuid":"2545f583fd5443bfd020381e82d8d55396e12de8"}},{"cell_type":"code","source":"thresh = 0.4 ### Threshold chosen from above Curves\ny_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\ny_pred = (y_pred_test_thresh > thresh).astype(int) \nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"_uuid":"103d4f25a79e368912cb498c82f2951cbdd3395d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression Confusion matrix is very similar to Decision Tree and Random Forest Classifier.\nIn this analysis, we did extensive analysis of input data and were able to achieve Test Accuracy of  __86 %__\n\n","metadata":{"_uuid":"9151235fbf3e118bfbe36ac56d0f30173262ac79"}},{"cell_type":"code","source":"","metadata":{"_uuid":"6be7724637fead13499c06da15cf18d5ff94b2bf"},"execution_count":null,"outputs":[]}]}